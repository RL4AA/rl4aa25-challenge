{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style=\"text-align: center; vertical-align: middle;\">\n",
    "    <span style=\"color: #B74F3D;\"> 3rd Reinforcement Learning for Autonomous Accelerators Workshop Hands-On Challenge</span>\n",
    "    <span style=\"color: #666666;\">: Beam Transverse Steering at ARES Linear Accelerator</span>\n",
    "</h1>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"img/rl4aa_logo.png\" alt=\"RL4AA Logo\" style=\"max-width: 12%; height: auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"color: #b51f2a\">Getting started</h2>\n",
    "\n",
    "- You will need **Python 3.12 or higher** to run this code &#x2757;\n",
    "- You will require about **1 GB of free disk space** &#x2757;\n",
    "- Make sure you have Git installed in your terminal &#x2757;\n",
    "\n",
    "Start by cloning locally the repository of the tutorial by running this command in your terminal:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/RL4AA/rl4aa25-tutorial.git\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"color: #b51f2a\">Installing virtual environment</h2>\n",
    "\n",
    "### Using Conda\n",
    "\n",
    "- If you don't have conda installed already, you can install the `miniconda` as [described here](https://docs.conda.io/projects/miniconda/en/latest/miniconda-install.html).\n",
    "- We recommend to install `miniconda` the day beforehand to avoid network overload during the tutorial &#x2757; &#x2757;\n",
    "\n",
    "Once `miniconda` is installed run this command in your terminal:\n",
    "\n",
    "```bash\n",
    "conda env create -f environment.yml\n",
    "```\n",
    "\n",
    "This should create a virtual environment named `rl25-tutorial` and install the necessary packages inside.\n",
    "\n",
    "Afterwards, activate the environment using\n",
    "\n",
    "```bash\n",
    "conda activate rl25-tutorial\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"color: #b51f2a\">Installing virtual environment</h2>\n",
    "\n",
    "### Using venv\n",
    "\n",
    "_If you don't have conda installed:_\n",
    "\n",
    "Alternatively, you can create the virtual env with\n",
    "\n",
    "```bash\n",
    "python3 -m venv rl-tutorial\n",
    "```\n",
    "\n",
    "and activate the env with `$ source <venv>/bin/activate` (bash) or `C:> <venv>/Scripts/activate.bat` (Windows)\n",
    "\n",
    "Then, install the packages with `pip` within the activated environment\n",
    "\n",
    "```bash\n",
    "python -m pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Afterwards, you should be able to run the provided scripts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"color: #b51f2a\">Check your installation</h2>\n",
    "If you set up your virtual environment correctly and is activated you should be able to run the next cell without any errors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import yaml\n",
    "from gymnasium.wrappers import RescaleAction\n",
    "from IPython.display import HTML, Latex, Markdown, clear_output, display\n",
    "\n",
    "from src.environments import ea\n",
    "from src.environments.ea_auxiliary import make_eval_env\n",
    "from src.utils import evaluate_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"color: #b51f2a\"> ARES (Accelerator Research Experiment at SINBAD)</h2>\n",
    "\n",
    "<p>ARES is an S-band radio frequency linac at the DESY Hamburg site equipped with a photoinjector and two independently driven traveling wave accelerating structures. The main research focus is the generation and characterization of sub-femtosecond electron bunches at relativistic particle energy. The generation of short electron bunches is of high interest for radiation generation, i.e. by free electron lasers.</p>\n",
    "\n",
    "<img src=\"img/ARES_layout.png\" style=\"width:100%; margin:auto;\"/>\n",
    "\n",
    "\n",
    "- **Final energy**: 100-155 MeV\n",
    "- **Bunch charge**: 0.01-200 pC\n",
    "- **Bunch length**: 30 fs - 1 ps\n",
    "- **Pulse repetition rate**: 1-50 Hz!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"color: #b51f2a\">The accelerator problem we want to solve</h2>\n",
    "\n",
    "<p>We would like to focus and center the electron beam on a diagnostic screen using corrector and quadrupole magnets</p>\n",
    "\n",
    "<img src=\"img/ares_magnets.png\" style=\"width:70%; margin:auto;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"color: #b51f2a\">Formulating the RL problem</h2>\n",
    "\n",
    "<h3>Overview of our study case</h3>\n",
    "\n",
    "<img src=\"img/ares_rl_problem.png\" style=\"width:70%; margin:auto;\"/>\n",
    "\n",
    "<h3 style=\"color:#038aa1;\">Discussion</h3>\n",
    "<p style=\"color:#038aa1;\"> $\\implies$  Is the action space continuous or discrete? </p>\n",
    "<p style=\"color:#038aa1;\"> $\\implies$  Is the problem fully observable or partially observable?</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Actions</h3>\n",
    "\n",
    "<div class=\"row\">\n",
    "    <div class=\"column\" style=\"width:60%;float:left\">\n",
    "        <p>In the ARES transverse tuning task we have 3 quadrupoles and 2 corrector magnets</p>\n",
    "        <p>The actions are:\n",
    "            <ul>\n",
    "            <li><b>Quadrupole magnet strength</b> $k_{1,2,3}$ $[1/m^2]$</li>\n",
    "            <li><b>Corrector deflection angle</b> $\\theta_\\mathrm{v, h}$ $[mrad]$ (vertical and horizontal</li>\n",
    "            </ul>\n",
    "        </p>\n",
    "        <p>In our control system we can set these derived values directly according the beam energy</p>\n",
    "        <p>$\\implies$ <code>actions</code> $=[k_{\\mathrm{Q1}},k_{\\mathrm{Q2}},\\theta_\\mathrm{CV},k_{\\mathrm{Q3}},\\theta_\\mathrm{CH}]$</p>\n",
    "            <p>is a 5-dimensional array</p>\n",
    "    </div>\n",
    "    <div class=\"column\" style=\"width:40%;float:right\">\n",
    "        <img src=\"img/dipole.png\" style=\"width:50%; margin:auto;\"/>\n",
    "        <img src=\"img/quads.png\" style=\"width:35%; margin:auto;\"/>\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Observation / state</h3>\n",
    "\n",
    "<div class=\"row\">\n",
    "    <div class=\"column\" style=\"width:50%;float:left\">\n",
    "        <p>Observation is the information an agent receives about the current state of the environment</p>\n",
    "        <p>It should provide enough information so that the agent can solve this problem.</p>\n",
    "        <p>The observation does not necessarily cover the entire (internal) state of the environment.</p>\n",
    "        <h3 style=\"color:#038aa1;\">Discussion</h3>\n",
    "        <p style=\"color:#038aa1;\"> $\\implies$ What should be included in the <code>observation</code>?  </p>\n",
    "        <p style=\"color:#038aa1;\"> $\\implies$ What can be observed in the simulation? </p>\n",
    "        <p style=\"color:#038aa1;\"> $\\implies$ What cannot be observed in the real world? </p>\n",
    "        <p style=\"color:#038aa1;\"> $\\implies$ How does this relate to the environment? </p>\n",
    "    </div>\n",
    "    <div class=\"column\" style=\"width:50%;float:right\">\n",
    "      <img src=\"img/screen_2.png\" style=\"width:40%; margin:auto;\"/>\n",
    "      <p style=\"clear:both; font-size: small; text-align: center; margin-top:1em;\">\n",
    "          The screen is made from scintillating material and glows when hit by electrons</p>\n",
    "      <img src=\"img/screen_1.png\" style=\"width:40%; margin:auto;\"/>\n",
    "      <p style=\"clear:both; font-size: small; text-align: center; margin-top:1em;\">The camera films the screen</p>\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3> The environment's state (continued)</h3>\n",
    "\n",
    "The `state` can be fully described by with four components:\n",
    "\n",
    "- The **target beam**: the beam we want to achieve, our goal\n",
    "  - as a 4-dimensional array $b^\\mathrm{(t)}=[\\mu_x^{(\\mathrm{t})},\\sigma_x^{(\\mathrm{t})},\\mu_y^{(\\mathrm{t})},\\sigma_y^{(\\mathrm{t})}]$, where $\\mu$ denotes the position on the screen, $\\sigma$ denotes the beam size, and $t$ stands for \"target\".\n",
    "- The **incoming beam**: the beam that enters the EA upstream\n",
    "  - $I = [\\mu_x^{(\\mathrm{i})},\\sigma_x^{(\\mathrm{i})},\\mu_y^{(\\mathrm{i})},\\sigma_y^{(\\mathrm{i})},\\mu_{xp}^{(\\mathrm{i})},\\sigma_{xp}^{(\\mathrm{i})},\\mu_{yp}^{(\\mathrm{i})},\\sigma_{yp}^{(\\mathrm{i})},\\mu_s^{(\\mathrm{i})},\\sigma_s^{(\\mathrm{i})}]$, where $i$ stands for \"incoming\"\n",
    "- The **magnet strengths** and **deflection angles**\n",
    "  - $[k_{\\mathrm{Q1}},k_{\\mathrm{Q2}},\\theta_\\mathrm{CV},k_{\\mathrm{Q3}},\\theta_\\mathrm{CH}]$\n",
    "- The **transverse misalignments** of **quadrupoles** and the **diagnostic screen**\n",
    "  - $[m_{\\mathrm{Q1}}^{(\\mathrm{x})},m_{\\mathrm{Q1}}^{(\\mathrm{y})},m_{\\mathrm{Q2}}^{(\\mathrm{x})},m_{\\mathrm{Q2}}^{(\\mathrm{y})},m_{\\mathrm{Q3}}^{(\\mathrm{x})},m_{\\mathrm{Q3}}^{(\\mathrm{y})},m_{\\mathrm{S}}^{(\\mathrm{x})},m_{\\mathrm{S}}^{(\\mathrm{y})}]$\n",
    "\n",
    "<h3 style=\"color:#038aa1;\">Discussion</h3>\n",
    "<p style=\"color:#038aa1;\"> $\\implies$ Do we (fully) know or can we observe the state of the environment?</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3> Our definition of observation</h3>\n",
    "\n",
    "The `observation` for this task consists of three components:\n",
    "\n",
    "- The **target beam**:  The desired beam, or the goal we aim to achieve.\n",
    "  - as a 4-dimensional array $b^\\mathrm{(t)}=[\\mu_x^{(\\mathrm{t})},\\sigma_x^{(\\mathrm{t})},\\mu_y^{(\\mathrm{t})},\\sigma_y^{(\\mathrm{t})}]$, where $\\mu$ represents the position on the screen, $\\sigma$ denotes the beam size, and $t$ refers to the \"target\".\n",
    "- The **current beam**: The beam currently in place.\n",
    "  - $b^\\mathrm{(c)}=[\\mu_x^{(\\mathrm{c})},\\sigma_x^{(\\mathrm{c})},\\mu_y^{(\\mathrm{c})},\\sigma_y^{(\\mathrm{c})}]$, where $c$ represents \"current\".\n",
    "- Magnet settings: The **magnet strengths** and **deflection angles**\n",
    "  - $[k_{\\mathrm{Q1}},k_{\\mathrm{Q2}},\\theta_\\mathrm{CV},k_{\\mathrm{Q3}},\\theta_\\mathrm{CH}]$\n",
    "\n",
    "<h3 style=\"color:#038aa1;\">Discussion</h3>\n",
    "<p style=\"color:#038aa1;\"> $\\implies$ Does this observation definition satisfy the Markov property? That is, does the probability distribution for the next beam depend only on the current observation, or is it influenced by other state information?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>RL Goal and reward</h3>\n",
    "\n",
    "Our goal is divided into two tasks:\n",
    "\n",
    "1. **Steering** the beam to the desired position.\n",
    "2. **Focusing** the beam to the desired size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"color: #b51f2a\">About libraries for RL</h2>\n",
    "\n",
    "<p>There are several libraries that provide pre-implemented RL algorithms and frameworks for creating environments. In this notebook, we use:</p>\n",
    "\n",
    "- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/) for the RL algorithms\n",
    "- [Gymnasium](https://gymnasium.farama.org/) for the environment\n",
    "<br>\n",
    "<img src=\"img/rl_libraries.png\"  style=\"width:60%; margin:auto;\"/>\n",
    "\n",
    "<p style=\"clear:both; font-size: small; text-align: center; margin-top:1em;\">More info <a href=\"https://neptune.ai/blog/the-best-tools-for-reinforcement-learning-in-python\">here</a></p>\n",
    "\n",
    "**Note**:\n",
    "\n",
    "- Gymnasium is the successor of the [OpenAI Gym](https://www.gymlibrary.dev/).\n",
    "- Stable-baselines3 now has an early-stage JAX implementation [sbx](https://github.com/araffin/sbx).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"color: #b51f2a\">A Gymnasium Environment</h2>\n",
    "\n",
    "We take all the elements of the RL problem we previously defined and represent the tuning task as a `gym`-based environment, a standard library for RL tasks.\n",
    "\n",
    "A custom `gym.Env` consists of the following components:\n",
    "\n",
    "- **Initialization**: Sets up the environment and defines the `observation_space` and `action_space`\n",
    "- `reset` **method**: Resets the environment for a new episode and returns a 2-tuple `(observation, info)`\n",
    "- `step` **method**: Contains the core logic. It accepts an action, updates the environment state, generates a new observation, computes the reward, and returns a 5-tuple `(observation, reward, terminated, truncated, info)`.\n",
    "  - `terminated` Determines whether the episode should end based on the underlying MDP (e.g., goal reached, threshold exceeded)\n",
    "  - `truncated` Checks if the episode should be truncated due to conditions outside the MDP (e.g., time limits).\n",
    "- `render` **method**: Provides a visual representation of the environment (e.g., video or plots)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"color: #b51f2a\">The ARES Experimental Area (ARES-EA) Environment</h2>\n",
    "\n",
    "- We formulated the ARES-EA task as a `gym` environment, allowing our algorithm to easily interface with both the simulation and real machine backends as shown before.\n",
    "  \n",
    "- In this section, you will become familiar with the environment for beam focusing and positioning at the ARES accelerator.\n",
    "\n",
    "Important APIs:\n",
    "\n",
    "- `reset`: Resets the magnets to their initial values in both real and simulation cases. In the simulation, it also regenerates the incoming beam and (optionally) resets magnet misalignments.\n",
    "- `step`: Adjusts the magnets to new settings and observes the beam (either by running a simulation or observing the screen image in the real world)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Interacting with the ARES-EA Environment</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Interacting live with 3D Rendering</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Try Manual Tuning</h3>\n",
    "\n",
    "TODO: Move manual_tuning notebook into here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>What to do next?</h2>\n",
    "\n",
    "To participate in the challenge, you can \n",
    "- try training an RL agent. We have provided a baseline training routine with PPO. C.f. `Challenge_A_RLTraining.ipynb`\n",
    "- try developing your own optimizer or controller, c.f. `Challenge_B_CustomOptimizer.ipynb`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #b51f2a\">Code Directory Structure in RL4AA-2025 Repository</h2>\n",
    "\n",
    "<p> This repository contains all the necessary code and configurations for running experiments using reinforcement learning (RL) and other numerical optimizers for the ARES-EA transverse tuning task. Below is an overview of the directories to help you navigate the code contents:</p>\n",
    "\n",
    "- `src` Contains the source code for the RL environment and the GP-MPC controller\n",
    "  - `src/environments/ea` contains the gymnasium environment for the ARES-EA transverse tuning task\n",
    "  - `src/wrappers` contains custom wrappers for the EA environment\n",
    "  - `src/train` contains scripts to train a default PPO agent to solve the task (can be used as a benchmark for evaluating MPC controller)\n",
    "- `data/trial.yaml` contains the pre-selected task configurations for evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl4aa25-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
